# Основные слои PyTorch

## Линейные слои

### nn.Linear

Полносвязный (fully connected) слой, выполняющий операцию `y = xW^T + b`. Каждый входной нейрон связан с каждым выходным через обучаемый вес. Это базовый строительный блок MLP (Multi-Layer Perceptron) и большинства архитектур в целом.

Принимает тензор произвольной формы, трансформируя только последнее измерение: `[batch, ..., in_features] -> [batch, ..., out_features]`. Матрица весов `W` имеет размер `[out_features, in_features]`, bias `b` — вектор длины `out_features`. Bias можно отключить параметром `bias=False`, что часто делают перед слоями нормализации.

---

## Свёрточные слои

### nn.Conv1d

Одномерная свёртка. Скользящее окно (ядро) проходит по одному измерению входного сигнала, вычисляя взвешенную сумму в каждой позиции. Применяется для обработки последовательностей: аудио, временных рядов, одномерных сигналов.

Ключевые параметры: `kernel_size` (размер окна), `stride` (шаг), `padding` (дополнение нулями по краям), `dilation` (разрежение ядра — позволяет увеличить рецептивное поле без увеличения числа параметров). Выход уменьшается в пространственном измерении в зависимости от stride и padding.

### nn.Conv2d

Двумерная свёртка — основной слой для обработки изображений. Ядро (например, 3x3) скользит по высоте и ширине входного тензора. Каждый выходной канал — это отдельный набор ядер, обучающийся детектировать определённый паттерн (грань, текстуру, форму).

Вход имеет форму `[batch, channels, height, width]`. Число параметров: `out_channels * in_channels * kernel_h * kernel_w + bias`. Например, Conv2d(3, 64, 3) для RGB-изображения создаст 64 фильтра по 3x3x3 = 27 весов каждый. Первые слои сети обычно детектируют низкоуровневые признаки (рёбра, градиенты), глубокие — высокоуровневые (объекты, части).

### nn.Conv3d

Трёхмерная свёртка. Ядро скользит по трём пространственным измерениям. Используется для видео (два пространственных измерения + время), медицинских 3D-снимков (КТ, МРТ) и воксельных данных.

Вычислительно значительно дороже Conv2d — число параметров и операций растёт кубически с размером ядра. На практике часто заменяют на (2+1)D свёртки: отдельно Conv2d по пространству и Conv1d по времени, что дешевле и часто работает не хуже.

### nn.ConvTranspose2d

"Обратная" свёртка (transposed convolution), также известна как deconvolution, хотя математически это не обратная операция к свёртке. Увеличивает пространственное разрешение: если Conv2d уменьшает изображение, ConvTranspose2d — увеличивает.

Применяется в генеративных моделях (GAN, VAE) для генерации изображений из латентного вектора, и в сегментации (U-Net) для восстановления пространственного разрешения после энкодера. Может создавать "шахматные артефакты" (checkerboard artifacts) из-за неравномерного перекрытия ядер — иногда лучше использовать `nn.Upsample` + обычную Conv2d.

---

## Рекуррентные слои

### nn.RNN

Базовый рекуррентный слой. На каждом шаге принимает вход `x_t` и скрытое состояние `h_{t-1}`, вычисляет новое скрытое состояние: `h_t = tanh(W_ih * x_t + W_hh * h_{t-1} + b)`. Таким образом, информация о предыдущих шагах "течёт" через скрытое состояние.

На практике почти не используется из-за проблемы vanishing/exploding gradients: при обратном распространении через много шагов градиенты экспоненциально затухают или взрываются. Сеть не может выучить долгосрочные зависимости (связь между словами, разделёнными десятками шагов). LSTM и GRU решают эту проблему.

### nn.LSTM

Long Short-Term Memory. Расширяет RNN механизмом "вентилей" (gates): forget gate (что забыть), input gate (что запомнить), output gate (что выдать). Дополнительно к скрытому состоянию `h` поддерживает cell state `c` — "долговременную память", через которую градиенты текут почти без затухания.

Формулы сложнее, чем у RNN (4 матричных умножения вместо одного), зато LSTM способен запоминать зависимости на сотни шагов. Долгое время был стандартом для NLP, машинного перевода, генерации текста. Сейчас в большинстве задач вытеснен трансформерами, но остаётся актуальным для временных рядов и задач с потоковой обработкой.

### nn.GRU

Gated Recurrent Unit — упрощённый LSTM. Два вентиля вместо трёх (reset gate и update gate), нет отдельного cell state. Меньше параметров, быстрее обучается.

На практике качество GRU и LSTM на большинстве задач сопоставимо. GRU чуть быстрее и проще, LSTM чуть лучше на задачах с очень длинными зависимостями. Выбор между ними — часто вопрос вкуса и эксперимента.

---

## Механизм внимания

### nn.MultiheadAttention

Реализация multi-head scaled dot-product attention. Вход делится на `num_heads` "голов", каждая вычисляет внимание независимо: `Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V`, затем результаты конкатенируются и проецируются линейным слоем.

Множество голов позволяет модели одновременно обращать внимание на разные аспекты: одна голова следит за синтаксисом, другая за семантикой, третья за позицией. Это ключевой компонент трансформеров. Поддерживает self-attention (Q=K=V из одного источника) и cross-attention (Q из одного источника, K и V из другого — например, в декодере, который "смотрит" на выход энкодера).

### nn.TransformerEncoderLayer

Готовый блок энкодера трансформера: self-attention + feed-forward network, с residual connections и layer normalization. Принимает последовательность, возвращает последовательность той же длины, но с обогащённым контекстом в каждой позиции.

Внутри: `LayerNorm -> MultiheadAttention -> Dropout -> Residual -> LayerNorm -> Linear -> Activation -> Linear -> Dropout -> Residual`. Несколько таких слоёв стекаются в `nn.TransformerEncoder`. Используется в BERT, Vision Transformer (ViT) и других моделях, где нужно энкодировать входную последовательность целиком.

### nn.TransformerDecoderLayer

Блок декодера трансформера: self-attention с каузальной маской (видит только предыдущие позиции) + cross-attention на выход энкодера + feed-forward. Используется для генерации: перевод, суммаризация, ответы на вопросы.

Каузальная маска — ключевое отличие от энкодера. При генерации позиция `t` может видеть только позиции `0..t-1`, но не будущие. GPT-подобные модели используют только декодер (без cross-attention), BART и T5 — полную связку энкодер-декодер.

---

## Функции активации

### nn.ReLU

Rectified Linear Unit: `f(x) = max(0, x)`. Простейшая нелинейная активация. Вычисляется за одно сравнение, градиент равен 0 или 1 — нет проблемы vanishing gradient для положительных значений.

Проблема: "мёртвые нейроны". Если нейрон попал в отрицательную зону, его градиент равен нулю, и он перестаёт обучаться навсегда. На практике это редко критично при правильной инициализации весов, но для надёжности существуют варианты (LeakyReLU, PReLU).

### nn.GELU

Gaussian Error Linear Unit: `f(x) = x * Phi(x)`, где Phi — CDF стандартного нормального распределения. В отличие от ReLU, не обрезает отрицательные значения жёстко, а плавно "гасит" их.

Стандартная активация в трансформерах (BERT, GPT, ViT). Эмпирически даёт чуть лучшие результаты, чем ReLU, в задачах NLP и vision transformers. Вычислительно дороже ReLU, но разница незаметна на фоне остальных операций в трансформере.

### nn.SiLU

Sigmoid Linear Unit, он же Swish: `f(x) = x * sigmoid(x)`. По свойствам похож на GELU — плавная, неограниченная сверху, ограниченная снизу. Предложена Google Brain, доказала эффективность в задачах компьютерного зрения.

Используется в EfficientNet, YOLOv5+ и других современных CNN-архитектурах. Как и GELU, не имеет проблемы мёртвых нейронов. Разница между SiLU и GELU на практике минимальна — выбор чаще определяется традициями конкретной области (SiLU в vision, GELU в NLP).

### nn.Sigmoid

Логистическая функция: `f(x) = 1 / (1 + exp(-x))`. Выход в диапазоне (0, 1). Используется когда нужна вероятность: бинарная классификация (последний слой), вентили в LSTM/GRU, attention weights.

Как активация скрытых слоёв давно не используется из-за vanishing gradient: производная максимум 0.25, при глубокой сети градиенты быстро затухают. Также выход не центрирован вокруг нуля, что замедляет обучение. Но для выходного слоя бинарной классификации — стандарт.

### nn.Tanh

Гиперболический тангенс: `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`. Выход в диапазоне (-1, 1). Центрирован вокруг нуля, что лучше для обучения, чем sigmoid.

Используется внутри LSTM/GRU (для нормализации cell state), иногда как финальная активация в генеративных моделях, где выход должен быть в [-1, 1]. Как активация скрытых слоёв уступает ReLU-семейству по той же причине — vanishing gradient.

### nn.Softmax

Преобразует вектор произвольных чисел в вектор вероятностей: `softmax(x_i) = exp(x_i) / sum(exp(x_j))`. Сумма выходов всегда равна 1, каждый элемент в (0, 1).

Стандартный выход для многоклассовой классификации. Параметр `dim` указывает, вдоль какого измерения нормализовать. В PyTorch часто не нужен явно — `nn.CrossEntropyLoss` включает softmax внутри (принимает "сырые" logits). Явный Softmax нужен при инференсе, в attention-механизмах и в ситуациях, где нужны именно вероятности.

### nn.LeakyReLU

Модификация ReLU: `f(x) = x if x > 0, else alpha * x` (обычно alpha=0.01). Для отрицательных значений пропускает маленький градиент вместо нуля, решая проблему мёртвых нейронов.

На практике разница с обычным ReLU часто незначительна. Популярен в GAN-ах (особенно в дискриминаторе), где мёртвые нейроны — более серьёзная проблема из-за нестабильности обучения.

---

## Нормализация

### nn.BatchNorm1d / nn.BatchNorm2d

Batch Normalization нормализует активации по батчу: для каждого канала вычисляет среднее и дисперсию по всем примерам в батче, нормализует, затем применяет обучаемые параметры масштаба (gamma) и сдвига (beta). BatchNorm1d для полносвязных слоёв, BatchNorm2d для свёрточных.

Ускоряет обучение (можно использовать больший learning rate), добавляет регуляризацию (статистики по батчу шумные). Проблемы: зависит от размера батча (маленький батч — шумные статистики), по-разному работает в train и eval (в eval использует скользящее среднее). Для трансформеров не подходит — там используют LayerNorm.

### nn.LayerNorm

Нормализация по признакам (features) внутри одного примера, а не по батчу. Для каждого примера независимо: вычисляет среднее и дисперсию по всем фичам, нормализует, масштабирует и сдвигает.

Стандарт для трансформеров. Не зависит от размера батча, одинаково работает в train и eval. Параметр `normalized_shape` — обычно размерность эмбеддинга (например, `LayerNorm(768)` для BERT-base).

### nn.GroupNorm

Компромисс между BatchNorm и LayerNorm. Каналы делятся на группы, нормализация происходит внутри каждой группы для каждого примера отдельно. Не зависит от размера батча, как LayerNorm, но сохраняет канальную структуру, как BatchNorm.

Хорошо работает в задачах компьютерного зрения с маленькими батчами (детекция объектов, сегментация). При `groups=1` эквивалентен LayerNorm, при `groups=num_channels` — Instance Normalization.

### nn.RMSNorm

Root Mean Square Normalization: упрощённый LayerNorm без вычитания среднего. Нормализует только по RMS (корень из среднего квадрата): `x / sqrt(mean(x^2) + eps) * gamma`. Нет параметра bias.

Используется в LLaMA, Gemma и других современных LLM. Чуть быстрее LayerNorm, качество сопоставимо. Стал де-факто стандартом для новых больших языковых моделей.

---

## Регуляризация

### nn.Dropout

Во время обучения случайно зануляет каждый элемент тензора с вероятностью `p`, остальные масштабирует на `1/(1-p)` для сохранения математического ожидания. При инференсе ничего не делает (поэтому важно переключать `model.eval()`).

Предотвращает переобучение: заставляет сеть не полагаться на конкретные нейроны, а распределять знание. Можно интерпретировать как ансамбль экспоненциально многих подсетей. Типичные значения: 0.1-0.3 для трансформеров, 0.5 для полносвязных сетей. Слишком высокий dropout — сеть не сможет обучиться, слишком низкий — не поможет с переобучением.

### nn.Dropout2d

Зануляет целые каналы (feature maps) вместо отдельных элементов. Для свёрточных сетей это более осмысленно: соседние пиксели в одном канале сильно коррелированы, и обычный Dropout неэффективен — информация "просачивается" через соседей.

Применяется между свёрточными слоями. Зануляя целый канал, заставляет сеть не полагаться на отдельные детекторы признаков.

---

## Пулинг

### nn.MaxPool2d

Берёт максимум в скользящем окне. Уменьшает пространственное разрешение (downsample), сохраняя самый сильный сигнал в каждой области. MaxPool2d(2) уменьшает размер в 2 раза по каждой оси.

Обеспечивает частичную инвариантность к сдвигу: если признак сместился на пару пикселей, максимум в окне всё равно его поймает. Не имеет обучаемых параметров. В современных архитектурах часто заменяется свёрткой со stride > 1, которая выучивает даунсэмплинг.

### nn.AvgPool2d

Берёт среднее в скользящем окне. Мягче, чем MaxPool: сохраняет общую статистику области вместо максимального сигнала. Менее популярен, чем MaxPool, для промежуточных слоёв.

Используется реже, но полезен в некоторых архитектурах (ResNet использует AvgPool перед финальным классификатором). Сглаживает активации, может терять мелкие детали.

### nn.AdaptiveAvgPool2d

Адаптивный пулинг: принимает нужный размер выхода, сам вычисляет размер окна и stride. `AdaptiveAvgPool2d((1, 1))` сжимает любое пространственное разрешение в один вектор — глобальное среднее по каждому каналу.

Незаменим для того, чтобы сеть принимала изображения произвольного размера. Без него: Conv-слои работают с любым размером, но Linear требует фиксированный вход. AdaptiveAvgPool2d((1, 1)) перед Linear решает эту проблему. Используется практически во всех современных классификаторах (ResNet, EfficientNet).

---

## Эмбеддинги

### nn.Embedding

Таблица поиска (lookup table): каждому целочисленному индексу (обычно — токену) сопоставляется обучаемый вектор фиксированной размерности. `Embedding(10000, 768)` — словарь из 10000 токенов, каждый представлен 768-мерным вектором.

Это основа работы с дискретными данными: слова, символы, категории. Технически эквивалентен one-hot encoding + Linear, но гораздо эффективнее (не создаёт разреженные матрицы). Вектора инициализируются случайно и обучаются вместе с остальной сетью, в процессе обучения семантически близкие токены получают близкие вектора.

### nn.EmbeddingBag

Эмбеддинг + агрегация за одну операцию. Принимает набор индексов и сразу возвращает их среднее, сумму или максимум, без промежуточного создания матрицы всех эмбеддингов.

Эффективнее, чем `Embedding` + ручная агрегация, для задач типа "представить документ как среднее эмбеддингов всех его слов". Используется в рекомендательных системах и моделях классификации текста (FastText).

---

## Вспомогательные слои

### nn.Flatten

Превращает многомерный тензор в плоский вектор. `Flatten()` преобразует `[batch, channels, height, width]` в `[batch, channels * height * width]`. Нужен для перехода от свёрточной части сети к полносвязной.

Параметры `start_dim` и `end_dim` позволяют выпрямить только часть измерений. По умолчанию выпрямляет все измерения кроме батча.

### nn.Unflatten

Обратная операция к Flatten: восстанавливает структуру из плоского вектора. `Unflatten(1, (32, 4, 4))` превращает `[batch, 512]` в `[batch, 32, 4, 4]`.

Используется в генеративных моделях: латентный вектор (1D) нужно развернуть в пространственную карту (3D), прежде чем подавать на ConvTranspose2d.

### nn.Sequential

Контейнер, который применяет слои последовательно: выход одного — вход следующего. Упрощает код: вместо ручного вызова каждого слоя в `forward()`, описываешь цепочку декларативно.

Ограничение: только линейные цепочки. Если нужны ветвления, skip-connections, несколько входов — Sequential не подходит, нужно писать `forward()` вручную.

### nn.ModuleList

Список слоёв, зарегистрированных как подмодули. В отличие от обычного Python-списка, ModuleList корректно регистрирует параметры — они видны для `model.parameters()`, сохраняются и загружаются с моделью.

Используется когда нужно динамическое количество слоёв или итерация по ним в цикле (например, N блоков трансформера). Не определяет порядок выполнения — это просто хранилище, порядок задаётся в `forward()`.

### nn.ModuleDict

Словарь слоёв, зарегистрированных как подмодули. Аналогичен ModuleList, но с доступом по строковым ключам вместо числовых индексов.

Удобен для условного выбора слоёв: `self.heads["classification"]` vs `self.heads["regression"]`. Также полезен для multi-task моделей, где каждая задача имеет свою "голову" (head) поверх общего backbone.
